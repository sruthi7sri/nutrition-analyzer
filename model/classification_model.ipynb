{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Part 1: Download, Clean, and Preprocess the Dataset\n",
    "# -------------------------------\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# ðŸ“Œ Step 1: Download Dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"openfoodfacts/world-food-facts\")\n",
    "print(\"Dataset downloaded successfully!\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Locate the TSV file\n",
    "tsv_files = [f for f in os.listdir(path) if f.endswith(\".tsv\")]\n",
    "if not tsv_files:\n",
    "    raise FileNotFoundError(\"No TSV files found in the dataset folder.\")\n",
    "\n",
    "# Load the TSV file (using the first found TSV)\n",
    "file_path = os.path.join(path, tsv_files[0])\n",
    "print(f\"Found dataset file: {file_path}\")\n",
    "df = pd.read_csv(file_path, delimiter=\"\\t\", low_memory=False)\n",
    "\n",
    "# ðŸ“Œ Step 2: Extract & Clean Data\n",
    "# Define the columns we care about\n",
    "columns_needed = [\n",
    "    \"product_name\", \"ingredients_text\", \"nutrition_grade_fr\", \n",
    "    \"energy_100g\", \"fat_100g\", \"sugars_100g\", \"salt_100g\"\n",
    "]\n",
    "df_filtered = df[columns_needed].copy()\n",
    "\n",
    "# Drop rows that are missing a nutrition grade (needed for labeling)\n",
    "df_filtered = df_filtered.dropna(subset=[\"nutrition_grade_fr\"])\n",
    "\n",
    "# For text columns, fill missing values with an empty string.\n",
    "text_cols = [\"product_name\", \"ingredients_text\"]\n",
    "df_filtered[text_cols] = df_filtered[text_cols].fillna(\"\")\n",
    "\n",
    "# For numeric columns, fill missing values with 0.\n",
    "numeric_cols = [\"energy_100g\", \"fat_100g\", \"sugars_100g\", \"salt_100g\"]\n",
    "df_filtered[numeric_cols] = df_filtered[numeric_cols].fillna(0)\n",
    "\n",
    "# Function to classify food based on nutrition grade\n",
    "def classify_health(nutrition_grade):\n",
    "    grade = nutrition_grade.lower().strip()\n",
    "    if grade in [\"a\", \"b\"]:\n",
    "        return \"healthy\"\n",
    "    elif grade == \"c\":\n",
    "        return \"moderately_healthy\"\n",
    "    else:\n",
    "        return \"unhealthy\"\n",
    "\n",
    "df_filtered[\"health_label\"] = df_filtered[\"nutrition_grade_fr\"].apply(classify_health)\n",
    "\n",
    "# Function to clean the ingredient text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()                 # Convert to lowercase\n",
    "    text = re.sub(r\"\\d+\", \"\", text)           # Remove numbers\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)        # Remove punctuation\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df_filtered[\"clean_ingredients\"] = df_filtered[\"ingredients_text\"].apply(clean_text)\n",
    "\n",
    "# (Optional) Remove any rows where \"clean_ingredients\" is empty after cleaning.\n",
    "df_filtered = df_filtered[df_filtered[\"clean_ingredients\"].str.strip() != \"\"]\n",
    "\n",
    "# Save preprocessed data as CSV files for training later\n",
    "train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=42, stratify=df_filtered[\"health_label\"])\n",
    "train_df.to_csv(\"distilbert_train_data.csv\", index=False)\n",
    "test_df.to_csv(\"distilbert_test_data.csv\", index=False)\n",
    "print(\"Data preprocessing complete. Training and testing datasets are saved.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Part 2: Fine-Tune DistilBERT on the Preprocessed Data (GPU-Optimized)\n",
    "# -------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ðŸ“Œ Step 1: Load Preprocessed Dataset (CSV)\n",
    "train_path = \"distilbert_train_data.csv\"\n",
    "test_path = \"distilbert_test_data.csv\"\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "# Map health labels to numerical values\n",
    "label_mapping = {\"healthy\": 0, \"moderately_healthy\": 1, \"unhealthy\": 2}\n",
    "df_train[\"label\"] = df_train[\"health_label\"].map(label_mapping)\n",
    "df_test[\"label\"] = df_test[\"health_label\"].map(label_mapping)\n",
    "\n",
    "# ðŸ“Œ Step 2: Tokenize the Text Data\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"clean_ingredients\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Apply tokenization (batched processing)\n",
    "train_dataset = train_dataset.map(lambda x: tokenize_function(x), batched=True)\n",
    "test_dataset = test_dataset.map(lambda x: tokenize_function(x), batched=True)\n",
    "\n",
    "# ðŸ“Œ Step 3: Define Model & Training Parameters (with GPU enhancements)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                         # Enable half-precision training for GPUs\n",
    "    dataloader_num_workers=4,          # Use multiple workers for faster data loading\n",
    "    report_to=\"none\"                   # Disable logging to WandB\n",
    ")\n",
    "\n",
    "# ðŸ“Œ Step 4: Define Metrics for Evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ðŸ“Œ Step 5: Fine-Tune the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ðŸ“Œ Step 6: Evaluate & Save the Model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "model.save_pretrained(\"distilbert_food_classifier\")\n",
    "tokenizer.save_pretrained(\"distilbert_food_classifier\")\n",
    "print(\"Model training complete and saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
